<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · JuliaLLMLeaderboard.jl</title><meta name="title" content="Reference · JuliaLLMLeaderboard.jl"/><meta property="og:title" content="Reference · JuliaLLMLeaderboard.jl"/><meta property="twitter:title" content="Reference · JuliaLLMLeaderboard.jl"/><meta name="description" content="Documentation for JuliaLLMLeaderboard.jl."/><meta property="og:description" content="Documentation for JuliaLLMLeaderboard.jl."/><meta property="twitter:description" content="Documentation for JuliaLLMLeaderboard.jl."/><meta property="og:url" content="https://svilupp.github.io/Julia-LLM-Leaderboard/reference/"/><meta property="twitter:url" content="https://svilupp.github.io/Julia-LLM-Leaderboard/reference/"/><link rel="canonical" href="https://svilupp.github.io/Julia-LLM-Leaderboard/reference/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">JuliaLLMLeaderboard.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../methodology/">Methodology</a></li><li><a class="tocitem" href="../test_definitions/">Test Definitions</a></li><li><span class="tocitem">Results</span><ul><li><a class="tocitem" href="../examples/summarize_results_paid/">Paid APIs</a></li><li><a class="tocitem" href="../examples/summarize_results_local/">Local Models</a></li><li><a class="tocitem" href="../examples/compare_paid_vs_local/">Paid vs Local Models</a></li><li><a class="tocitem" href="../examples/summarize_results_prompts/">By Prompts</a></li><li><a class="tocitem" href="../examples/summarize_results_test_cases/">By Test Case</a></li></ul></li><li><a class="tocitem" href="../frequently_asked_questions/">F.A.Q.</a></li><li class="is-active"><a class="tocitem" href>Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/svilupp/Julia-LLM-Leaderboard" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/main/docs/src/reference.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><ul><li><a href="#InteractiveUtils.edit"><code>InteractiveUtils.edit</code></a></li><li><a href="#JuliaLLMLeaderboard.evaluate_1shot-Tuple{}"><code>JuliaLLMLeaderboard.evaluate_1shot</code></a></li><li><a href="#JuliaLLMLeaderboard.find_definitions"><code>JuliaLLMLeaderboard.find_definitions</code></a></li><li><a href="#JuliaLLMLeaderboard.load_conversation_from_eval-Tuple{AbstractString}"><code>JuliaLLMLeaderboard.load_conversation_from_eval</code></a></li><li><a href="#JuliaLLMLeaderboard.load_definition-Tuple{Any}"><code>JuliaLLMLeaderboard.load_definition</code></a></li><li><a href="#JuliaLLMLeaderboard.load_evals-Tuple{AbstractString}"><code>JuliaLLMLeaderboard.load_evals</code></a></li><li><a href="#JuliaLLMLeaderboard.preview-Tuple{PromptingTools.AbstractMessage}"><code>JuliaLLMLeaderboard.preview</code></a></li><li><a href="#JuliaLLMLeaderboard.preview-Tuple{AbstractVector{&lt;:PromptingTools.AbstractMessage}}"><code>JuliaLLMLeaderboard.preview</code></a></li><li><a href="#JuliaLLMLeaderboard.run_benchmark-Tuple{}"><code>JuliaLLMLeaderboard.run_benchmark</code></a></li><li><a href="#JuliaLLMLeaderboard.run_code_blocks_additive-Tuple{AICode, AbstractVector{&lt;:AbstractString}}"><code>JuliaLLMLeaderboard.run_code_blocks_additive</code></a></li><li><a href="#JuliaLLMLeaderboard.run_code_main-Tuple{AIMessage}"><code>JuliaLLMLeaderboard.run_code_main</code></a></li><li><a href="#JuliaLLMLeaderboard.save_definition-Tuple{AbstractString, AbstractDict}"><code>JuliaLLMLeaderboard.save_definition</code></a></li><li><a href="#JuliaLLMLeaderboard.score_eval-Tuple{AbstractDict}"><code>JuliaLLMLeaderboard.score_eval</code></a></li><li><a href="#JuliaLLMLeaderboard.score_eval-NTuple{4, Any}"><code>JuliaLLMLeaderboard.score_eval</code></a></li><li><a href="#JuliaLLMLeaderboard.timestamp_now-Tuple{}"><code>JuliaLLMLeaderboard.timestamp_now</code></a></li><li><a href="#JuliaLLMLeaderboard.tmapreduce-Tuple{Any, Any, Any}"><code>JuliaLLMLeaderboard.tmapreduce</code></a></li><li><a href="#JuliaLLMLeaderboard.validate_definition-Tuple{AbstractDict}"><code>JuliaLLMLeaderboard.validate_definition</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="InteractiveUtils.edit" href="#InteractiveUtils.edit"><code>InteractiveUtils.edit</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">InteractiveUtils.edit(conversation::AbstractVector{&lt;:PT.AbstractMessage}, bookmark::Int=-1)</code></pre><p>Opens the conversation in a preview window formatted as markdown (In VSCode, right click on the tab and select &quot;Open Preview&quot; to format it nicely).</p><p>See also: <code>preview</code> (for rendering as markdown in REPL)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/conversations.jl#L96-L103">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.evaluate_1shot-Tuple{}" href="#JuliaLLMLeaderboard.evaluate_1shot-Tuple{}"><code>JuliaLLMLeaderboard.evaluate_1shot</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">evaluate_1shot(; conversation, fn_definition, definition, model, prompt_label, schema, parameters::NamedTuple=NamedTuple(), device=&quot;UNKNOWN&quot;, timestamp=timestamp_now(), version_pt=string(pkgversion(PromptingTools)), prompt_strategy=&quot;1SHOT&quot;, verbose::Bool=false,
auto_save::Bool=true, save_dir::AbstractString=dirname(fn_definition), experiment::AbstractString=&quot;&quot;,
execution_timeout::Int=60, capture_stdout::Bool=true)</code></pre><p>Runs evaluation for a single test case (parse, execute, run examples, run unit tests), including saving the files.</p><p>If <code>auto_save=true</code>, it saves the following files</p><ul><li><code>&lt;model-name&gt;/evaluation__PROMPTABC__1SHOT__TIMESTAMP.json</code></li><li><code>&lt;model-name&gt;/conversation__PROMPTABC__1SHOT__TIMESTAMP.json</code> </li></ul><p>into a sub-folder of where the definition file was stored.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>conversation</code>: the conversation to evaluate (vector of messages), eg, from <code>aigenerate</code> when <code>return_all=true</code></li><li><code>fn_definition</code>: path to the definition file (eg, <code>joinpath(&quot;code_generation&quot;, &quot;utility_functions&quot;, &quot;event_scheduler&quot;, &quot;definition.toml&quot;)</code>)</li><li><code>definition</code>: the test case definition dict loaded from the definition file. It&#39;s subset to only the relevant keys for code generation, eg, <code>definition=load_definition(fn_definition)[&quot;code_generation&quot;]</code></li><li><code>model</code>: the model name, eg, <code>model=&quot;gpt4t&quot;</code></li><li><code>prompt_label</code>: the prompt label, eg, <code>prompt_label=&quot;JuliaExpertAsk&quot;</code></li><li><code>schema</code>: the schema used for the prompt, eg, <code>schema=&quot;-&quot;</code> or <code>schema=&quot;OllamaManagedSchema()&quot;</code></li><li><code>parameters</code>: the parameters used for the generation like <code>temperature</code> or <code>top_p</code>, eg, <code>parameters=(; top_p=0.9)</code></li><li><code>device</code>: the device used for the generation, eg, <code>device=&quot;Apple-MacBook-Pro-M1&quot;</code></li><li><code>timestamp</code>: the timestamp used for the generation. Defaults to <code>timestamp=timestamp_now()</code> which is like &quot;20231201_120000&quot;</li><li><code>version_pt</code>: the version of PromptingTools used for the generation, eg, <code>version_pt=&quot;0.1.0&quot;</code></li><li><code>prompt_strategy</code>: the prompt strategy used for the generation, eg, <code>prompt_strategy=&quot;1SHOT&quot;</code>. Fixed for now!</li><li><code>verbose</code>: if <code>verbose=true</code>, it will print out more information about the evaluation process, eg, the evaluation errors</li><li><code>auto_save</code>: if <code>auto_save=true</code>, it will save the evaluation and conversation files into a sub-folder of where the definition file was stored.</li><li><code>save_dir</code>: the directory where the evaluation and conversation files are saved. Defaults to <code>dirname(fn_definition)</code>.</li><li><code>experiment</code>: the experiment name, eg, <code>experiment=&quot;my_experiment&quot;</code> (eg, when you&#39;re doing a parameter search). Defaults to <code>&quot;&quot;</code> for standard benchmark run.</li><li><code>execution_timeout</code>: the timeout for the AICode code execution in seconds. Defaults to 60s.</li><li><code>capture_stdout</code>: if <code>capture_stdout=true</code>, AICode will capture the stdout of the code execution. Set to <code>false</code> if you&#39;re evaluating with multithreading (stdout capture is not thread-safe). Defaults to <code>true</code> to avoid poluting the benchmark.</li><li><code>remove_tests</code>: if <code>remove_tests=true</code>, AICode will remove any @testset blocks and unit tests from the main code definition (shields against model defining wrong unit tests inadvertedly).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JuliaLLMLeaderboard
using PromptingTools

fn_definition = joinpath(&quot;code_generation&quot;, &quot;utility_functions&quot;, &quot;event_scheduler&quot;, &quot;definition.toml&quot;)
d = load_definition(fn_definition)

msg = aigenerate(:JuliaExpertAsk; ask=d[&quot;code_generation&quot;][&quot;prompt&quot;], model=&quot;gpt4t&quot;, return_all=true)

# Try evaluating it -- auto_save=false not to polute our benchmark
evals = evaluate_1shot(; conversation=msg, fn_definition, definition=d[&quot;code_generation&quot;], model=&quot;gpt4t&quot;, prompt_label=&quot;JuliaExpertAsk&quot;, timestamp=timestamp_now(), device=&quot;Apple-MacBook-Pro-M1&quot;, schema=&quot;-&quot;, prompt_strategy=&quot;1SHOT&quot;, verbose=true, auto_save=false)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/evaluation.jl#L174-L220">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.find_definitions" href="#JuliaLLMLeaderboard.find_definitions"><code>JuliaLLMLeaderboard.find_definitions</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Finds all <code>definition.toml</code> filenames in the given path. Returns a list of filenames to load.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/definitions.jl#L167">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.load_conversation_from_eval-Tuple{AbstractString}" href="#JuliaLLMLeaderboard.load_conversation_from_eval-Tuple{AbstractString}"><code>JuliaLLMLeaderboard.load_conversation_from_eval</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Loads the conversation from the corresponding evaluation file.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/conversations.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.load_definition-Tuple{Any}" href="#JuliaLLMLeaderboard.load_definition-Tuple{Any}"><code>JuliaLLMLeaderboard.load_definition</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Loads the test case definition from a TOML file under <code>filename</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/definitions.jl#L162">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.load_evals-Tuple{AbstractString}" href="#JuliaLLMLeaderboard.load_evals-Tuple{AbstractString}"><code>JuliaLLMLeaderboard.load_evals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">load_evals(base_dir::AbstractString; score::Bool=true, max_history::Int=5, new_columns::Vector{Symbol}=Symbol[], kwargs...)</code></pre><p>Loads all evaluation JSONs from a given director loaded in a DataFrame as rows.  The directory is searched recursively, and all files starting with the prefix <code>evaluation__</code> are loaded.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>score::Bool=true</code>: If <code>score=true</code>, the function will also call <code>score_eval</code> on the resulting DataFrame.</li><li><code>max_history::Int=5</code>: Only <code>max_history</code> most recent evaluations are loaded. If <code>max_history=0</code>, all evaluations are loaded.</li></ul><p>Returns: DataFrame</p><p>Note: It loads a fixed set of columns (set in a local variable <code>eval_cols</code>), so if you added some new columns, you&#39;ll need to pass them to <code>new_columns::Vector{Symbol}</code> argument.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/evaluation.jl#L331-L344">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.preview-Tuple{AbstractVector{&lt;:PromptingTools.AbstractMessage}}" href="#JuliaLLMLeaderboard.preview-Tuple{AbstractVector{&lt;:PromptingTools.AbstractMessage}}"><code>JuliaLLMLeaderboard.preview</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">preview(conversation::AbstractVector{&lt;:PT.AbstractMessage})</code></pre><p>Render a conversation, which is a vector of <code>AbstractMessage</code> objects, as a single markdown-formatted string. Each message is rendered individually and concatenated with separators for clear readability.</p><p>This function is particularly useful for displaying the flow of a conversation in a structured and readable format. It leverages the <code>PT.preview</code> method for individual messages to create a cohesive view of the entire conversation.</p><p><strong>Arguments</strong></p><ul><li><code>conversation::AbstractVector{&lt;:PT.AbstractMessage}</code>: A vector of messages representing the conversation.</li></ul><p><strong>Returns</strong></p><ul><li><code>String</code>: A markdown-formatted string representing the entire conversation.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">conversation = [
    PT.SystemMessage(&quot;Welcome&quot;),
    PT.UserMessage(&quot;Hello&quot;),
    PT.AIMessage(&quot;Hi, how can I help you?&quot;)
]
println(PT.preview(conversation))</code></pre><p>This will output:</p><pre><code class="nohighlight hljs"># System Message
Welcome
---
# User Message
Hello
---
# AI Message
Hi, how can I help you?
---</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/conversations.jl#L53-L88">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.preview-Tuple{PromptingTools.AbstractMessage}" href="#JuliaLLMLeaderboard.preview-Tuple{PromptingTools.AbstractMessage}"><code>JuliaLLMLeaderboard.preview</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">preview(msg::PT.AbstractMessage)</code></pre><p>Render a single <code>AbstractMessage</code> as a markdown-formatted string, highlighting the role of the message sender and the content of the message.</p><p>This function identifies the type of the message (User, Data, System, AI, or Unknown) and formats it with a header indicating the sender&#39;s role, followed by the content of the message. The output is suitable for nicer rendering, especially in REPL or markdown environments.</p><p><strong>Arguments</strong></p><ul><li><code>msg::PT.AbstractMessage</code>: The message to be rendered.</li></ul><p><strong>Returns</strong></p><ul><li><code>String</code>: A markdown-formatted string representing the message.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">msg = PT.UserMessage(&quot;Hello, world!&quot;)
println(PT.preview(msg))</code></pre><p>This will output:</p><pre><code class="nohighlight hljs"># User Message
Hello, world!</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/conversations.jl#L7-L31">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.run_benchmark-Tuple{}" href="#JuliaLLMLeaderboard.run_benchmark-Tuple{}"><code>JuliaLLMLeaderboard.run_benchmark</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">run_benchmark(; fn_definitions::Vector{&lt;:AbstractString}=find_definitons(joinpath(@__DIR__, &quot;..&quot;, &quot;code_generation&quot;)),
models::Vector{String}=[&quot;gpt-3.5-turbo-1106&quot;], model_suffix::String=&quot;&quot;, prompt_labels::Vector{&lt;:AbstractString}=[&quot;JuliaExpertCoTTask&quot;, &quot;JuliaExpertAsk&quot;, &quot;InJulia&quot;, &quot;AsIs&quot;, &quot;JuliaRecapTask&quot;, &quot;JuliaRecapCoTTask&quot;],
api_kwargs::NamedTuple=NamedTuple(), http_kwargs::NamedTuple=(; readtimeout=300),
experiment::AbstractString=&quot;&quot;, save_dir::AbstractString=&quot;&quot;, auto_save::Bool=true, verbose::Union{Int,Bool}=true, device::AbstractString=&quot;-&quot;,
num_samples::Int=1, schema_lookup::AbstractDict{String,&lt;:Any}=Dict{String,&lt;:Any}())</code></pre><p>Runs the code generation benchmark with specified models and prompts for the specified number of samples.</p><p>It will generate the response, evaluate it and, optionally, also save it.</p><p>Note: This benchmark is not parallelized, because locally hosted models would be overwhelmed.  However, you can take this code and apply <code>Threads.@spawn</code> to it. The evaluation itself should be thread-safe.</p><p><strong>Keyword Arguments</strong></p><ul><li><p><code>fn_definitions</code>: a vector of paths to definitions of test cases to run (<code>definition.toml</code>). If not specified, it will run all definition files in the <code>code_generation</code> folder.</p></li><li><p><code>models</code>: a vector of models to run. If not specified, it will run only <code>gpt-3.5-turbo-1106</code>.</p></li><li><p><code>model_suffix</code>: a string to append to the model name in the evaluation data, eg, &quot;–optim&quot; if you provide some tuned API kwargs.</p></li><li><p><code>prompt_labels</code>: a vector of prompt labels to run. If not specified, it will run all available.</p></li><li><p><code>num_samples</code>: an integer to specify the number of samples to generate for each model/prompt combination. If not specified, it will generate 1 sample.</p></li><li><p><code>api_kwargs</code>: a named tuple of API kwargs to pass to the <code>aigenerate</code> function. If not specified, it will use the default values. Example: <code>(; temperature=0.5, top_p=0.5)</code></p></li><li><p><code>http_kwargs</code>: a named tuple of HTTP.jl kwargs to pass to the <code>aigenerate</code> function. Defaults to 300s timeout. Example: <code>http_kwargs = (; readtimeout=300)</code></p></li><li><p><code>schema_lookup</code>: a dictionary of schemas to use for each model. If not specified, it will use the default schemas in the registry. Example: <code>Dict(&quot;mistral-tiny&quot; =&gt; PT.MistralOpenAISchema())</code></p></li><li><p><code>codefixing_num_rounds</code>: an integer to specify the number of rounds of codefixing to run (with AICodeFixer). If not specified, it will NOT be used.</p></li><li><p><code>codefixing_prompt_labels</code>: a vector of prompt labels to use for codefixing. If not specified, it will default to only &quot;CodeFixerTiny&quot;.</p></li><li><p><code>experiment</code>: a string to save in the evaluation data. Useful for future analysis.</p></li><li><p><code>device</code>: specify the device the benchmark is running on, eg, &quot;Apple-MacBook-Pro-M1&quot; or &quot;NVIDIA-GTX-1080Ti&quot;, broadly &quot;manufacturer-model&quot;.</p></li><li><p><code>save_dir</code>: a string of path where to save the evaluation data. If not specified, it will save in the same folder as the definition file. Useful for small experiments not to pollute the main benchmark.</p></li><li><p><code>auto_save</code>: a boolean whether to automatically save the evaluation data. If not specified, it will save the data. Otherwise, it only returns the vector of <code>evals</code></p></li><li><p><code>verbose</code>: a boolean or integer to print progress. If not specified, it will print highlevel progress (verbose = 1 or true), for more details, set <code>verbose=2 or =3</code>.</p></li><li><p><code>execution_timeout</code>: an integer to specify the timeout in seconds for the execution of the generated code. If not specified, it will use 10s (per run/test item).</p></li></ul><p><strong>Return</strong></p><ul><li>Vector of <code>evals</code>, ie, a dictionary of evaluation data for each model/prompt combination and each sample.</li></ul><p><strong>Notes</strong></p><ul><li>In general, use HTTP timeouts because both local models and APIs can get stuck, eg, <code>http_kwargs=(; readtimeout=150)</code></li><li>On Mac M1 with Ollama, you want to set api<em>kwargs=(; options=(; num</em>gpu=99)) for Ollama to have normal performance (ie, offload all model layers to the GPU)</li><li>For commercial providers (MistralAI, OpenAI), we automatically inject a different random seed for each run to avoid caching</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">fn_definitions = find_definitions(&quot;code_generation/&quot;) # all test cases

evals = run_benchmark(; fn_definitions, models=[&quot;gpt-3.5-turbo-1106&quot;], prompt_labels=[&quot;JuliaExpertAsk&quot;],
    experiment=&quot;my-first-run&quot;, save_dir=&quot;temp&quot;, auto_save=true, verbose=true, device=&quot;Apple-MacBook-Pro-M1&quot;,
    num_samples=1);

# not using `schema_lookup` as it&#39;s not needed for OpenAI models</code></pre><p>Or if you want only one test case use: <code>fn_definitions = [joinpath(&quot;code_generation&quot;, &quot;utility_functions&quot;, &quot;event_scheduler&quot;, &quot;definition.toml&quot;)]</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/workflow.jl#L1-L57">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.run_code_blocks_additive-Tuple{AICode, AbstractVector{&lt;:AbstractString}}" href="#JuliaLLMLeaderboard.run_code_blocks_additive-Tuple{AICode, AbstractVector{&lt;:AbstractString}}"><code>JuliaLLMLeaderboard.run_code_blocks_additive</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">run_code_blocks_additive(cb::AICode, code_blocks::AbstractVector{&lt;:AbstractString};
    verbose::Bool = false,
    setup_code::AbstractString = &quot;&quot;, teardown_code::AbstractString = &quot;&quot;,
    capture_stdout::Bool = true, execution_timeout::Int = 60)</code></pre><p>Runner for the additional <code>code_blocks</code> (can be either unit tests or examples), returns count of examples executed without an error. </p><p><code>code_blocks</code> should be a vector of strings, each of which is a valid Julia expression that can be evaluated without an error thrown. Each successful run (no error thrown) is counted as a successful example.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>verbose=true</code> will provide more information about the test failures.</li><li><code>setup_code</code> is a string that will be prepended to each code block before it&#39;s evaluated. Useful for setting up the environment/test objects.</li><li><code>teardown_code</code> is a string that will be appended to each code block before it&#39;s evaluated. Useful for cleaning up the environment/test objects.</li><li><code>capture_stdout</code> is a boolean whether to capture the stdout of the code execution. Set to <code>false</code> if you&#39;re evaluating with multithreading (stdout capture is not thread-safe).</li><li><code>execution_timeout</code> is the timeout for the AICode code execution in seconds. Defaults to 60s.</li></ul><p><strong>Returns</strong></p><ul><li><code>count_successful</code> the number of examples that were executed without an error thrown.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using JuliaLLMLeaderboard: run_code_blocks
using PromptingTools: AICode

cb = AICode(&quot;mysum(a,b)=a+b&quot;)
code = &quot;mysum(1,2)&quot;
run_code_blocks(cb, [code])
# Output: 1 (= 1 example executed without an error thrown)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/evaluation.jl#L90-L121">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.run_code_main-Tuple{AIMessage}" href="#JuliaLLMLeaderboard.run_code_main-Tuple{AIMessage}"><code>JuliaLLMLeaderboard.run_code_main</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">run_code_main(msg::PT.AIMessage; verbose::Bool = true, function_name::AbstractString = &quot;&quot;,
    prefix::String = &quot;&quot;,
    execution_timeout::Int = 60,
    capture_stdout::Bool = true,
    expression_transform::Symbol = :remove_all_tests)</code></pre><p>Runs the code block in the message <code>msg</code> and returns the result as an <code>AICode</code> object.</p><p>Logic:</p><ul><li>Always execute with a timeout</li><li>Always execute in a &quot;safe mode&quot; (inside a custom module, <code>safe_eval=true</code>)</li><li>Skip any package imports or environment changes (<code>skip_unsafe=true</code>)</li><li>Skip invalid/broken lines (<code>skip_invalid=true</code>)</li><li>Remove any unit tests (<code>expression_transform=:remove_all_tests</code>), because model might have added some without being asked for it explicitly</li><li>First, evaluate the code block as a whole, and if it fails, try to extract the function definition and evaluate it separately (fallback)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/evaluation.jl#L10-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.save_definition-Tuple{AbstractString, AbstractDict}" href="#JuliaLLMLeaderboard.save_definition-Tuple{AbstractString, AbstractDict}"><code>JuliaLLMLeaderboard.save_definition</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Saves the test case <code>definition</code> to a TOML file under <code>filename</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/definitions.jl#L148">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.score_eval-NTuple{4, Any}" href="#JuliaLLMLeaderboard.score_eval-NTuple{4, Any}"><code>JuliaLLMLeaderboard.score_eval</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">score_eval(parsed, executed, unit_tests_success_ratio, examples_success_ratio; max_points::Int=100)</code></pre><p>Score the evaluation result by distributing <code>max_points</code> equally across the available criteria.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">df=@rtransform df :score = score_eval(:parsed, :executed, :unit_tests_passed / :unit_tests_count, :examples_executed / :examples_count)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/evaluation.jl#L443-L452">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.score_eval-Tuple{AbstractDict}" href="#JuliaLLMLeaderboard.score_eval-Tuple{AbstractDict}"><code>JuliaLLMLeaderboard.score_eval</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">score_eval(eval::AbstractDict; max_points::Int=100)

score_eval(parsed, executed, unit_tests_success_ratio, examples_success_ratio; max_points::Int=100)</code></pre><p>Scores the evaluation result <code>eval</code> by distributing <code>max_points</code> equally across the available criteria. Alternatively, you can provide the individual scores as arguments (see above) with values in the 0-1 range.</p><p>Eg, if all 4 criteria are available, each will be worth 25% of points:</p><ul><li><code>parsed</code> (25% if true)</li><li><code>executed</code> (25% if true)</li><li><code>unit_tests</code> (25% if all unit tests passed)</li><li><code>examples</code> (25% if all examples executed without an error thrown)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/evaluation.jl#L411-L424">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.timestamp_now-Tuple{}" href="#JuliaLLMLeaderboard.timestamp_now-Tuple{}"><code>JuliaLLMLeaderboard.timestamp_now</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Provide a current timestamp in the format yyyymmdd<em>HHMMSS. If `add</em>random` is true, a random number between 100 and 999 is appended to avoid overrides.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/utils.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.tmapreduce-Tuple{Any, Any, Any}" href="#JuliaLLMLeaderboard.tmapreduce-Tuple{Any, Any, Any}"><code>JuliaLLMLeaderboard.tmapreduce</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tmapreduce(f, op, itr; tasks_per_thread::Int = 2, kwargs...)</code></pre><p>A parallelized version of the <code>mapreduce</code> function leveraging multi-threading.</p><p>The function <code>f</code> is applied to each element of <code>itr</code>, and then the results are reduced using an associative two-argument function <code>op</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: A function to apply to each element of <code>itr</code>.</li><li><code>op</code>: An associative two-argument reduction function.</li><li><code>itr</code>: An iterable collection of data.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>tasks_per_thread::Int = 2</code>: The number of tasks spawned per thread. Determines the granularity of parallelism.</li><li><code>kwargs...</code>: Additional keyword arguments to pass to the inner <code>mapreduce</code> calls.</li></ul><p><strong>Implementation Details</strong></p><p>The function divides <code>itr</code> into chunks, spawning tasks for processing each chunk in parallel. The size of each chunk is determined by <code>tasks_per_thread</code> and the number of available threads (<code>nthreads</code>). The results from each task are then aggregated using the <code>op</code> function.</p><p><strong>Notes</strong></p><p>This implementation serves as a general replacement for older patterns. The goal is to introduce this function or a version of it to base Julia in the future.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Base.Threads: nthreads, @spawn
result = tmapreduce(x -&gt; x^2, +, 1:10)</code></pre><p>The above example squares each number in the range 1 through 10 and then sums them up in parallel.</p><p>Source: <a href="https://julialang.org/blog/2023/07/PSA-dont-use-threadid/#better_fix_work_directly_with_tasks">Julia Blog post</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/utils.jl#L22-L53">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JuliaLLMLeaderboard.validate_definition-Tuple{AbstractDict}" href="#JuliaLLMLeaderboard.validate_definition-Tuple{AbstractDict}"><code>JuliaLLMLeaderboard.validate_definition</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">validate_definition(definition::AbstractDict; evaluate::Bool=true, verbose::Bool=true)</code></pre><p>Validates the <code>definition.toml</code> file for the code generation benchmark. </p><p>Returns <code>true</code> if the definition is valid.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>evaluate</code>: a boolean whether to evaluate the definition. If not specified, it will evaluate the definition.</li><li><code>verbose</code>: a boolean whether to print progress during the evaluation. If not specified, it will print progress.</li><li><code>kwargs</code>: keyword arguments to pass to code parsing function (<code>PT.AICode</code>).</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">fn_definition = joinpath(&quot;code_generation&quot;, &quot;utility_functions&quot;, &quot;event_scheduler&quot;, &quot;definition.toml&quot;)
definition = load_definition(fn_definition)
validate_definition(definition)
# output: true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/8735c07e231a064b8ed6c1154f333799ced5eb90/src/definitions.jl#L20-L39">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../frequently_asked_questions/">« F.A.Q.</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Tuesday 14 May 2024 06:52">Tuesday 14 May 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
